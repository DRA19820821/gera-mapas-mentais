# backend/services/llm_factory.py
"""
Factory para cria√ß√£o de inst√¢ncias de LLMs.

Fornece uma interface unificada para trabalhar com diferentes
provedores de LLM (OpenAI, Anthropic, Google, DeepSeek) usando
as integra√ß√µes nativas do LangChain.
"""

from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_deepseek import ChatDeepSeek
from typing import Optional
import os

from ..core.config import get_settings
from ..utils.logger import logger


# ============================================
# MAPEAMENTO DE MODELOS PADR√ÉO
# ============================================

DEFAULT_MODELS = {
    "openai": "gpt-4.1",
    "anthropic": "claude-sonnet-4-20250514",
    "gemini": "gemini-2.5-pro",
    "deepseek": "deepseek-reasoner"
}


# ============================================
# FUN√á√ÉO PRINCIPAL - FACTORY
# ============================================

def get_llm(
    provider: str,
    model: Optional[str] = None,
    temperature: float = 0.7,
    max_tokens: int = 4000,
    **kwargs
):
    """
    Factory para obter inst√¢ncia de LLM configurada.
    
    Usa as integra√ß√µes nativas do LangChain para cada provider,
    garantindo compatibilidade total com recursos como structured output,
    streaming, e function calling.
    
    Args:
        provider: Nome do provider (openai, anthropic, gemini, deepseek)
        model: Nome do modelo (opcional, usa padr√£o do provider)
        temperature: Temperatura para gera√ß√£o (0.0 a 1.0)
        max_tokens: M√°ximo de tokens na resposta
        **kwargs: Argumentos adicionais espec√≠ficos do provider
    
    Returns:
        ChatModel: Inst√¢ncia do modelo configurado
    
    Raises:
        ValueError: Se provider for inv√°lido ou n√£o configurado
    
    Examples:
        >>> llm = get_llm("anthropic", temperature=0.3)
        >>> response = llm.invoke("Hello!")
        
        >>> llm = get_llm("openai", model="gpt-4-turbo")
        >>> response = await llm.ainvoke([{"role": "user", "content": "Hi"}])
    """
    
    settings = get_settings()
    provider = provider.lower()
    
    # Valida provider
    valid_providers = ["openai", "anthropic", "gemini", "deepseek"]
    if provider not in valid_providers:
        raise ValueError(
            f"Provider '{provider}' n√£o √© v√°lido. "
            f"Op√ß√µes: {', '.join(valid_providers)}"
        )
    
    # Verifica se provider est√° configurado
    if not settings.is_provider_configured(provider):
        configured = settings.list_configured_providers()
        raise ValueError(
            f"Provider '{provider}' n√£o est√° configurado no .env\n"
            f"Providers configurados: {', '.join(configured) if configured else 'nenhum'}\n"
            f"Adicione a API key correspondente no arquivo .env"
        )
    
    # Usa modelo padr√£o se n√£o especificado
    if model is None:
        model = DEFAULT_MODELS[provider]
    
    logger.debug(
        f"Criando LLM: provider={provider}, model={model}, "
        f"temp={temperature}, max_tokens={max_tokens}"
    )
    
    # ============================================
    # OPENAI
    # ============================================
    
    if provider == "openai":
        return ChatOpenAI(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.openai_api_key,
            timeout=settings.llm_timeout,
            **kwargs
        )
    
    # ============================================
    # ANTHROPIC
    # ============================================
    
    elif provider == "anthropic":
        return ChatAnthropic(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.anthropic_api_key,
            timeout=settings.llm_timeout,
            **kwargs
        )
    
    # ============================================
    # GOOGLE GEMINI
    # ============================================
    
    elif provider == "gemini":
        return ChatGoogleGenerativeAI(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            google_api_key=settings.google_api_key,
            timeout=settings.llm_timeout,
            **kwargs
        )
    
    # ============================================
    # DEEPSEEK
    # ============================================
    
    elif provider == "deepseek":
        return ChatDeepSeek(
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            api_key=settings.deepseek_api_key,
            timeout=settings.llm_timeout,
            **kwargs
        )
    
    else:
        # N√£o deve chegar aqui devido √† valida√ß√£o anterior
        raise ValueError(f"Provider n√£o implementado: {provider}")


# ============================================
# FUN√á√ïES AUXILIARES
# ============================================

def list_available_providers() -> list[str]:
    """
    Lista providers que est√£o configurados e dispon√≠veis.
    
    Returns:
        list: Lista de nomes de providers configurados
    """
    settings = get_settings()
    return settings.list_configured_providers()


def get_default_model(provider: str) -> str:
    """
    Retorna o modelo padr√£o de um provider.
    
    Args:
        provider: Nome do provider
    
    Returns:
        str: Nome do modelo padr√£o
    
    Raises:
        ValueError: Se provider for inv√°lido
    """
    provider = provider.lower()
    
    if provider not in DEFAULT_MODELS:
        raise ValueError(
            f"Provider '{provider}' n√£o existe. "
            f"Op√ß√µes: {', '.join(DEFAULT_MODELS.keys())}"
        )
    
    return DEFAULT_MODELS[provider]


def validate_provider_config(provider: str) -> tuple[bool, str]:
    """
    Valida se um provider est√° corretamente configurado.
    
    Args:
        provider: Nome do provider
    
    Returns:
        tuple: (is_valid, error_message)
    """
    settings = get_settings()
    return settings.validate_provider(provider)


def get_provider_info(provider: str) -> dict:
    """
    Retorna informa√ß√µes sobre um provider.
    
    Args:
        provider: Nome do provider
    
    Returns:
        dict: Informa√ß√µes do provider
    """
    settings = get_settings()
    provider = provider.lower()
    
    if provider not in DEFAULT_MODELS:
        return {
            "provider": provider,
            "exists": False,
            "configured": False,
            "default_model": None
        }
    
    return {
        "provider": provider,
        "exists": True,
        "configured": settings.is_provider_configured(provider),
        "default_model": DEFAULT_MODELS[provider],
        "api_key_set": bool(settings.get_provider_key(provider))
    }


# ============================================
# TESTE DA FACTORY
# ============================================

async def test_llm_factory():
    """
    Testa a factory com todos os providers configurados.
    
    √ötil para validar configura√ß√£o durante desenvolvimento.
    """
    from ..utils.logger import logger
    
    logger.info("üß™ Testando LLM Factory...")
    
    providers = list_available_providers()
    
    if not providers:
        logger.error("‚ùå Nenhum provider configurado!")
        return False
    
    logger.info(f"üìã Providers configurados: {', '.join(providers)}")
    
    success = True
    
    for provider in providers:
        try:
            logger.info(f"\nüîç Testando {provider}...")
            
            # Cria LLM
            llm = get_llm(provider, temperature=0.5, max_tokens=100)
            
            # Testa invoca√ß√£o simples
            response = await llm.ainvoke("Say 'Hello' in one word")
            
            logger.success(f"‚úÖ {provider}: {response.content[:50]}")
            
        except Exception as e:
            logger.error(f"‚ùå {provider} falhou: {str(e)}")
            success = False
    
    if success:
        logger.success("\nüéâ Todos os providers funcionando!")
    else:
        logger.warning("\n‚ö†Ô∏è Alguns providers falharam")
    
    return success


# ============================================
# SCRIPT DE TESTE
# ============================================

if __name__ == "__main__":
    import asyncio
    
    print("="*60)
    print("LLM FACTORY - TESTE")
    print("="*60)
    
    # Lista providers dispon√≠veis
    print("\nüìã Providers dispon√≠veis:")
    for provider in DEFAULT_MODELS.keys():
        info = get_provider_info(provider)
        status = "‚úÖ Configurado" if info["configured"] else "‚ùå N√£o configurado"
        print(f"  {provider}: {status} (modelo: {info['default_model']})")
    
    # Testa factory
    print("\nüß™ Executando testes...")
    asyncio.run(test_llm_factory())